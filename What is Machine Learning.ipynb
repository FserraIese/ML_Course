{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What is Machine Learning?\n",
    "\n",
    "## Miguel Ángel Canela, IESE Business School\n",
    "\n",
    "******\n",
    "\n",
    "### The concept\n",
    "\n",
    "**Machine learning** (ML) is a branch of **artificial intelligence** (AI). You may have heard about other branches, such as robotics, or speech recognition. The objective of machine learning is the development and implementation of algorithms that learn from data how to accomplish difficult or tiring tasks. The process of deriving an algorithm from the data is called **training**. Sometimes, an algorithm derived from a set of **training data** is validated on data which have not been involved in the obtention of the algorithm, referred to as **testing data**.\n",
    "\n",
    "Nowadays, machine learning and artificial intelligence are no longer arcane subjects, and they are getting popular in the business world. Algorithms are regarded as something common in many organizations, and the ability of developing, maintaining and optimizing them is frequently included in job requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised and unsupervised learning\n",
    "\n",
    "In machine learning, based on the structure of the data used for learning, it is usual to distinguish between supervised and unsupervised learning. Roughly speaking, **supervised learning** is what the statisticians call prediction, that is, the description of one variable ($Y$), in terms of other variables (the $X$'s). In the machine learning context, $Y$ is called **target**, and the $X$'s are called **features**. The term **regression** applies to the prediction of a numeric target, and the term **classification** to the prediction of a categorical one. In **binary classification**, the target takes only two values, while in **multi-class classification** can take any number of values. These values are called **classes**.\n",
    "\n",
    "In an example of regression, we may try to predict the price of a house from a set of attributes of that house. In one of classification, to predict whether a customer is going to quit, from his/her demographics plus some measures of customer activity.\n",
    "\n",
    "In **unsupervised leaning**, there is no target to be predicted (only $X$'s). The objective is to learn patterns from the data. Unsupervised learning is more difficult, and more creative, than supervised learning. The two classics of unsupervised learning are **clustering**, which consists in grouping objects based on their similarity, and **association rules mining**, which consists in extracting from the data rules such as *if A, then B*. A typical application of clustering in business is **customer segmentation**. Association rules are applied in **market basket analysis**, to associate products that are purchased (or viewed in a website) together. Other examples of unsupervised learning are **dimensionality reduction** and **anomaly detection**.\n",
    "\n",
    "In-between supervised and unsupervised learning, we have **semisupervised learning**. Also, **reinforcement learning**, which is one of the trending ML topics, because of its surprising success in playing games like go and StarCraft II, cannot be considered neither as supervised nor as unsupervised learning. For more information, see Chapter 1 of Géron (2017).\n",
    "\n",
    "From the point of view of the practical implementation, we have also to distinguish between batch and on-line learning. In **batch learning**, the algorithm is trained and tested on given data sets and applied for some time without modification. In **on-line training**, it is retrained continuously with the incoming data. The choice between batch and continuous learning depends on practical issues, rather on theoretical arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing supervised learning algorithms\n",
    "\n",
    "Irrespective of the algorithm employed, the implementation of supervised learning procedures requires some steps to be taken to make the algorithms learn and work properly. Statisticians deal with this as follows. The sample is assumed to be statistically representative of a population. Then, the results obtained on the sample are accepted as estimates of unknown population parameters. Of course, this assumption is risky, since, in many real cases, the sample generation process is not under control by the analyst. \n",
    "\n",
    "Machine learning practitioners are not so optimistic. They accept that, since the learning algorithm is trained on the sample, the algorithm learns from the data and adapts to them. So, they introduce a second step, in which the trained algorithm is tested on new data. To get honest estimates of the algorithm's performance, it is extremely important for the test sample to be different from the training sample. This may be achieved in many ways, e.g., by splitting the original sample in two parts and using only one for the training step, or using more sophisticated **cross validation** procedures. The final aim is, for the trained algorithm, to have good generalization capabilities, so great care must be given in avoiding **overfitting**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main approaches to supervised learning\n",
    "\n",
    "1. **Decision trees** and **random forests**. A decision tree is a classification/regression algorithm, which learns from the data how to partition the feature space in such a way that, within each element of the partition, the target is as homogeneous as possible. When the target value of a new instance is to be predicted, that instance is assigned to a subset of the partition and the predicted target value is the one associated to this subset. Decision trees are exposed to overfitting. A more effective procedure, the random forest algorithm, generalizes the tree approach. A random forest is a set of decision trees, each grown independently of the others. When a new input is passed to the forest, each tree predicts target value. The random forest then predicts the target value as the class most frequently predicted by the ensemble of trees (classification) or by averaging trees' predictions (regression). Even if each tree of the random forest is a weak predictor, the tree ensemble is a stronger one and, more importantly, it suffers less from overfitting.\n",
    "\n",
    "2. **Boosting**. Boosting is a general and effective method of producing an accurate classifier, which tends not to overfit, by combining moderately inaccurate classifiers. The basic idea behind boosting can be outlined as follows: (i) choose a base learning algorithm (e.g., a decision tree) and train it on a sample, (ii) repeat the training process, by attaching higher weights to the training instances wrongly classified during the preceding step, so the learning algorithm tends to specialize to those harder cases, and (iii) repeat these rounds $n$ times, producing a sequence of $n$ weak classifiers. The final classification rule is obtained by counting/averaging the outcomes provided by the set of weak classifiers, weighted by the weights associated with each round. \n",
    "\n",
    "3. **Support vector machines** (SVM). A SVM is a classification algorithm which separates classes by linear expressions. As, in general, it is not possible to separate classes in such a way, based on the available features, SVM maps the instances into a new, higher-dimensional feature space, where separation can be indeed achieved in a linear way. The key point in SVM is that it is not necessary to explicitly build the map to the new feature space, as the classification criterion can be built just as a function of the so-called **kernel**, which expresses the inner products between pairs of transformed vectors. SVMs were initially developed for binary classification, but have later been extended.\n",
    "\n",
    "4. **Bayesian classifiers**. A Bayesian classifier is an algorithm which estimates the probability of an instance to belong to one out of $k$ classes, given a set of features and a set of assumptions on the conditional probabilities of the features, given the classes. Different models for the conditional and marginal probabilities lead to different classifiers. Although based on the oversimplified assumption of conditional independence of the features given the classes, one of the most effective Bayesian classifiers is the so-called **naive Bayesian classifier**.\n",
    "\n",
    "5. **Neural networks**. Neural networks are tools for nonlinear modeling. In their simplest form, they can be considered as two-stage regression or classification models, where input data are linearly combined into nonobserved features; these are then used to predict the target value with another equation. Neural networks can be described as graphs, composed of various **layers** of nodes connected by edges. Various kinds of architectures exist which adapt to different tasks. The coefficients of the equations forming the model must be tuned to fit the data. This employs a method called **backpropagation**. Neural networks are prone to overfit the data.\n",
    "\n",
    "6. **Deep learning**. New advances on neural networks with many hidden layers have turned the so called deep learning into the rock star of machine learning. It is currently employed in computer vision, speech recognition, and natural language processing, but also for tasks such as the implementation of recommendation systems in e-commerce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. E Alpaydin (2016), *Machine Learning*, MIT Press.\n",
    "\n",
    "2. F Chollet (2017), *Deep Learning with Python*, Manning.\n",
    "\n",
    "3. P Domingos (2015), *The Master Algorithm*, Bascic Books.\n",
    "\n",
    "4. A Géron (2017), *Hands-On Machine Learning with Scikit-Learn & TensorFlow*, O'Reilly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
