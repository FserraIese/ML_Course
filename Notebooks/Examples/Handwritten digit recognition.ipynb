{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digit recognition\n",
    "\n",
    "## Miguel √Ångel Canela, IESE Business School\n",
    "\n",
    "******\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This example deals with the classification of grayscale images of handwritten digits (28 pixels by 28 pixels), into 10 classes (0 to 9). The data set used is the **MNIST data set**, a classic in the machine learning community, which has been around for almost as long as the field itself and has been very intensively studied. It is a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (NIST) in the 1980s. You can think of \"solving\" MNIST as the \"Hello World\" of deep learning, that is, what you do to verify that your algorithms are working as expected. As you become a machine learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on.\n",
    "\n",
    "\n",
    "### Importing the data\n",
    "\n",
    "Since the original data set was too big to be posted on GitHub in CSV format, it has been partitioned into seven subsets. I import these subsets one by one (this can take a bit, depending on your connection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "folder = 'https://raw.githubusercontent.com/mcanela-iese/ML_Course/master/Data/'\n",
    "df1 = pd.read_csv(folder + 'digits1.csv')\n",
    "df2 = pd.read_csv(folder + 'digits2.csv')\n",
    "df3 = pd.read_csv(folder + 'digits3.csv')\n",
    "df4 = pd.read_csv(folder + 'digits4.csv')\n",
    "df5 = pd.read_csv(folder + 'digits5.csv')\n",
    "df6 = pd.read_csv(folder + 'digits6.csv')\n",
    "df7 = pd.read_csv(folder + 'digits7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I concatenate these data frames with the Pandas function `concat`. The argument `axis=0` means that they are concatenated vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df7], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check that the shape of the data set is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row in this data set stands for an image. Let us explore the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 70000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      "label    70000 non-null int64\n",
      "X1x1     70000 non-null int64\n",
      "X1x2     70000 non-null int64\n",
      "X1x3     70000 non-null int64\n",
      "X1x4     70000 non-null int64\n",
      "X1x5     70000 non-null int64\n",
      "X1x6     70000 non-null int64\n",
      "X1x7     70000 non-null int64\n",
      "X1x8     70000 non-null int64\n",
      "X1x9     70000 non-null int64\n",
      "dtypes: int64(10)\n",
      "memory usage: 5.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.iloc[:, :10].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first column must be a label identifying the digit, and the other 784 correspond to the image pixels (28 x 28 = 784). The name `Xixj` must be read as the gray intensity of the pixel in row `i` and column `j`. I split this into a **features matrix** and a **target vector** as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the target vector are the 10 digit labels. The data set is a bit unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([6903, 7877, 6990, 7141, 6824, 6313, 6876, 7293, 6825, 6958]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the feature matrix are integers from 0 to 255, which stand for gray intensities (8-bit grayscale), from Black = 0 to White = 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data are typically normalized into the 0-1 range, so I do it, even if it is not needed for the actual purpose of this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digit recognition is a **multi-class** classification problem. Since that task calls for a model with some complexity, it is safer to set a test set for **validation** purposes. I follow the tradition of using 10,000 images of the MNIST data set for testing. For the split, I use the scikit-learn class `model_selection.train_test_split`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    model_selection.train_test_split(X, y, test_size=1/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting an image\n",
    "\n",
    "Before addressing the classification task, let me show you what these images are. Images can be plotted with Matplotlib in a very easy way. I start importing the module `pyplot` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to specify that we want a black and white picture. One of them is to set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of our features matrix corresponds to a 28 x 28 image. I have to reshape it into a matrix of 28 rows and 28 columns. In NumPy, this is easily done with the function `reshape`. The matrix returned is written row by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic = X[0, :].reshape(28,28)\n",
    "pic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `pyplot`, the function `imshow` renders the image, in black and white, due to the above specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `pic` is a matrix of numbers in the 0-1 range, so we can get easily get the negative. This evoques the number as written by a pencil on a white sheet of paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(1-pic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier\n",
    "\n",
    "I start with a **decision tree classifier**. I use the scikit-learn class `tree.DecisionTreeClassifier`, setting the maximum number of leaves at 128 ($2^7=128$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=128,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "treeclf = tree.DecisionTreeClassifier(max_leaf_nodes=128)\n",
    "treeclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I evaluate the model by means of the accuracy. Since there are 10 digits, the **baseline accuracy**, obtained by guessing at random, is about 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.805"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(treeclf.score(X_train, y_train), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation on the test set shows an acceptable degree of overfitting. You can improve the accuracy a bit with a bigger tree, but the validation becomes less convincing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.791"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(treeclf.score(X_test, y_test), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifier\n",
    "\n",
    "**Ensemble methods** are available in the scikit-learn module `ensemble`. The classes `RandomForestClassifier` and `RandomForestRegressor` provide **random forest** algorithms, which combine a collection of decision trees to improve the predictions. To train a random forest classifier with the MNIST data, I keep the size of the tree used above. If you try other values for the arguments of `RandomForestClassifier`, you will find that the number of leaves (`max_leaf_nodes`) matters more than the number of trees (`n_estimators`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=128,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "rfclf = ensemble.RandomForestClassifier(max_leaf_nodes=128, n_estimators=10)\n",
    "rfclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the forest improves significantly the performance of a single tree, with no symptoms of overfitting (in general, random forest models are less prone to overfitting than their competitors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rfclf.score(X_train, y_train), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.897"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rfclf.score(X_test, y_test), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting classifier\n",
    "\n",
    "The **gradient boosting** algorithm is used in both regression and classification. As the random forest algorithm, it is an ensemble method, which derives its predictions from a collection of decision trees. There is a difference between the random forest and the gradient boosting methods. The trees of the random forest are independent but, in the gradient boosting method, they are developed sequentially, which makes the learning process much slower. \n",
    "\n",
    "In scikit-learn, the class `ensemble.GradientBoostingClassifier` can be used to develop a gradient boosting classifier. But, instead of the scikit-learn library, I use here the class `XGBClassifier`, from the `xgboost` module. The growth of the trees is controlled in `XGBClassifier` as in scikit-learn. The default is `max_depth=3`. I keep the defaults of `n_estimators` and `learning_rate`, which makes sense for a first experience. This may take one hour in your computer, so you have time for lunch. If you want to speed it, you can decrease the number of trees, but this will affect the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbclf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "xgbclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default specification, gradient boosting improves the results of the random forest, although it comes with a shade of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.945"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(xgbclf.score(X_train, y_train), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.936"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(xgbclf.score(X_test, y_test), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "\n",
    "I explore next whether all this can work after a **dimensionality reduction**. In scikit-learn, the module `decomposition` provides several methods for dimensionality reduction. I apply the **principal components analysis** (PCA) algorithm, which extracts a new, smaller, set of features by means of a linear transformation. PCA is available in the class `PCA`.\n",
    "\n",
    "I set the number of components (the new features) at 25, which is drastic reduction. Note that this is **unsupervised learning**, so the method `fit` requires one argument, not two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=25, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=25)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every component accounts for a certain proportion of the total variance. They come sorted by that proportion. We get this with the method `explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09746116, 0.07155445, 0.06149531, 0.05403385, 0.04888934,\n",
       "       0.04305227, 0.03278262, 0.02889642, 0.02758364, 0.0234214 ,\n",
       "       0.02106689, 0.02037553, 0.01707064, 0.0169402 , 0.01583382,\n",
       "       0.01486345, 0.01319356, 0.01278988, 0.01187257, 0.01152926,\n",
       "       0.0106594 , 0.01009617, 0.0095898 , 0.00908934, 0.00883205])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative proportion helps to see whether we are right retaining a number of components. Here, 25 components account for 69% of the total variance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09746116, 0.16901561, 0.23051091, 0.28454476, 0.3334341 ,\n",
       "       0.37648637, 0.40926898, 0.4381654 , 0.46574904, 0.48917044,\n",
       "       0.51023733, 0.53061286, 0.5476835 , 0.5646237 , 0.58045752,\n",
       "       0.59532097, 0.60851452, 0.6213044 , 0.63317697, 0.64470624,\n",
       "       0.65536563, 0.66546181, 0.67505161, 0.68414095, 0.692973  ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `transform` creates a new training set and a new test set, with 25 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 25)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PC_train = pca.transform(X_train)\n",
    "PC_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 25)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PC_test = pca.transform(X_test)\n",
    "PC_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decision tree classifier revisited\n",
    "\n",
    "I develop now a decision tree classifier based on the PC features, to examine how the dimensionality reduction has affected the performance. I do not the constraint for the tree growth, so `treeclf` has the same meaning as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.801"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeclf.fit(PC_train, y_train)\n",
    "round(treeclf.score(PC_train, y_train), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.785"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(treeclf.score(PC_test, y_test), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the accuracy loss is mere 1% (it can be a bit more if you are unlucky with your test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The random forest classifier revisited\n",
    "\n",
    "I repeat the exercise with the random forest classifier. Now the accuracy loss looks more relevant. It may be that having less room for choosing the features involved in every tree affects the performance of the random forest more than when we have a single tree and we can pick all the available features. By increasing the number of trees, you can improve the current accuracy one or two points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.867"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfclf = ensemble.RandomForestClassifier(max_leaf_nodes=128, n_estimators=10)\n",
    "rfclf.fit(PC_train, y_train)\n",
    "round(rfclf.score(PC_train, y_train), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rfclf.score(PC_test, y_test), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient boosting classifier revisited\n",
    "\n",
    "We come back now to the gradient boosting classifier. Again, this will take a while, almost one hour in my Mac. There is a loss of accuracy, similar to that of the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbclf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "xgbclf.fit(PC_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.917"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(xgbclf.score(PC_train, y_train), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(xgbclf.score(PC_test, y_test), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
