{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Zalando pics - 2nd round\n",
    "\n",
    "## Miguel √Ångel Canela, IESE Business School\n",
    "\n",
    "******\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This example continues with the development of neural network classifiers for the Zalando images. In the data set, every row stands for an image.  \n",
    "\n",
    "### Importing the data\n",
    "\n",
    "The data come in seven parts, which I import one by one and put them together with the Pandas function `concat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# folder = 'https://raw.githubusercontent.com/mcanela-iese/ML_Course/master/Data/'\n",
    "folder = ''\n",
    "df1 = pd.read_csv(folder + 'zalando1.csv')\n",
    "df2 = pd.read_csv(folder + 'zalando2.csv')\n",
    "df3 = pd.read_csv(folder + 'zalando3.csv')\n",
    "df4 = pd.read_csv(folder + 'zalando4.csv')\n",
    "df5 = pd.read_csv(folder + 'zalando5.csv')\n",
    "df6 = pd.read_csv(folder + 'zalando6.csv')\n",
    "df7 = pd.read_csv(folder + 'zalando7.csv')\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df7], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I split the data set into a **features matrix** and a **target vector**, normalizing the features in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = df.iloc[:, 1:].values\n",
    "X = X/255\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the data with `train_test_split`, keeping 10,000 images for testing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    model_selection.train_test_split(X, y, test_size=1/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our former model in Keras\n",
    "\n",
    "The specification of neural network architectures in deep learning implementations is necessarily more complex than in libraries which are limited to MLP networks, due to the extra possibilities offered. So long, Keras seems to offer the simplest approach. \n",
    "\n",
    "In my first example, I come back to the MLP architecture which I already tried in scikit-learn, consisting in a single hidden layer with 32 nodes. I import the three modules that I actualyy need, `utils`, `models` and `layers`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import utils, models, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify a classifier in Keras, the target vector has to be transformed To specify a classifier in Keras, the target vector has to be transformed into a matrix in which each column is a dummy associated to one of the target values. This can be done in many ways, using Pandas or scikit-learn. In Keras itself, it is done with the function `to_categorical`, from `utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = utils.to_categorical(y_train, dtype=int)\n",
    "y_test = utils.to_categorical(y_test, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the class `Sequential` of the module `models` to specify the network architecture. I start initializing the class, with the default specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfclf1 = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ordinary MLP architecture, the layers added are specified with the class `layers`. In this first model, the hidden layer and the output layer are **dense**, that is, every node is connected to all nodes of the preceding layer. \n",
    "\n",
    "In the first layer that I create, which will be the hidden layer, I have to specify the shape of the input tensor, which is the number of features. In the forthcoming layers, the input is determined by the number of nodes of the preceding layer. In Keras, the different types of layers have different activation defaults (in a dense layer, `activation=None`), so it is always safer to specify the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfclf1.add(layers.Dense(32, input_shape=(784, ), activation='relu'))\n",
    "tfclf1.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network architecture is completely specified, the model is compiled, with the method `compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfclf1.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the method `fit`, which is just a bit more complex than in scikit-learn. The **number of epochs** is typically low, to prevent overfitting. The default is `epochs=1`. The **batch size** is typically set as a power of 2. The default is 32, although 64 is more popular. Note that these defaults are different from those of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1110 18:29:51.688523 4486792640 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.5996 - accuracy: 0.7924\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.4415 - accuracy: 0.8435\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.4084 - accuracy: 0.8548\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3862 - accuracy: 0.8627\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.3705 - accuracy: 0.8683\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.3565 - accuracy: 0.8724\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.3458 - accuracy: 0.8767\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.3365 - accuracy: 0.8799\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.3289 - accuracy: 0.8816\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.3224 - accuracy: 0.8843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17f72d610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfclf1.fit(X_train, y_train, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I evaluate the model in the test set. Note that, even with a very low number of epochs, we can overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 45us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3755474271059036, 0.8684999942779541]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfclf1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional network \n",
    "\n",
    "**2D convolutional neural networks** are the state of the art of image classification. They combine several types of layers, including dense layers. They have two parts, with different types of layers. In the first part, the input tensors, called here **feature maps**, are three-dimensional, with two spatial axes (`height` and `width`) as well as a `channels` axis. For black and white pictures like those of this example, there is only one channel.\n",
    "\n",
    "So, I start by reshaping the data so that we can feed the network with 60,000 tensors of shape (28, 28, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(60000, 28, 28, 1), X_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I initialize the class `Sequential`, in order to specify the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfclf2 = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the network is a stack of alternate `Conv2D` and `MaxPooling2D` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:31:16.627098 4486792640 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfclf2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "tfclf2.add(layers.MaxPooling2D((2, 2)))\n",
    "tfclf2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "tfclf2.add(layers.MaxPooling2D((2, 2)))\n",
    "tfclf2.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `summary` produces a report showing the shape of the output tensors and the number of parameters for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tfclf2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us follow the track. The input of the first convolutional layer has shape (28, 28, 1). The convolution works in 3 x 3 patches, so we lose one unit on each border. The 28 x 28 grid is transformed into a 26 x 26 grid in each of the 32 nodes, which means that the output tensor has shape (28, 28, 32).\n",
    "\n",
    "A `MaxPooling2D` layer extracts a window, typically a 2 x 2 window, and outputs the maximum value. This reduces the number of parameters and induces spatial-filter hierarchy. In the above summary, we see that, in the first `MaxPooling2D` layer, the 26 x 26 grids of the input tensor are transformed into 13 x 13 grids. \n",
    "\n",
    "The last layer of this stack outputs a tensor of shape (3, 3, 64). The second part of the network is a stack of `Dense` layers. Since these layers process vectors, which are 1D, I have to flatten the 3D inputs to 1D. This is done with a `Flatten` layer. There is no calculation in this layers, just a reshape.\n",
    "\n",
    "In this case, I have just added a `Dense` layers of 64 nodes, before the output layer, which is the same as in the MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfclf2.add(layers.Flatten())\n",
    "tfclf2.add(layers.Dense(64, activation='relu'))\n",
    "tfclf2.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tfclf2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest (compilation, fitting and evaluation) is as in the dense network. Five epochs is typically enough, given that these algorithms are prone to overfitting, as we have seen in our first algorithm, with a unique hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.5387 - accuracy: 0.7999\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 70s 1ms/step - loss: 0.3366 - accuracy: 0.8770\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 61s 1ms/step - loss: 0.2828 - accuracy: 0.8979\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 60s 1ms/step - loss: 0.2504 - accuracy: 0.9085\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 67s 1ms/step - loss: 0.2277 - accuracy: 0.9176\n",
      "10000/10000 [==============================] - 3s 295us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2653323025226593, 0.9072999954223633]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfclf2.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "tfclf2.fit(X_train, y_train, epochs=5, batch_size=64)\n",
    "tfclf2.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
