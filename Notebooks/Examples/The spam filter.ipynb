{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The spam filter\n",
    "\n",
    "## Miguel √Ångel Canela, IESE Business School\n",
    "\n",
    "******\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this example, I develop a **spam filter**, that is, an algorithm which classifies e-mail messages as either spam or non-spam, based on a collection of **numeric features** such as the frequency of certain words or characters. The algorithm is based on a **decision tree** obtained with the class `DecisionTreeClassifier` from the scikit-learn module `tree`. An alternative analysis can be performed with the class `DecisionTreeRegressor`, but I leave this for the homework. In this example, I have to take into account that the **false positive rate**, that is, the proportion of non-spam messages wrongly classified as spam, must be very low in an acceptable spam filter. \n",
    "\n",
    "I use data collected at Hewlett-Packard by merging: (a) a collection of spam e-mail from the company postmaster and the individuals who had filed spam, and (b) a collection of non-spam e-mail, extracted from filed work and personal e-mail. The data set contains data on 4,601 e-mail messages. Among these messages, 1,813 have been classified as spam. The variables are:\n",
    "\n",
    "* 48 numeric features whose names start with 'word_', followed by a word. They indicate the **frequency**, in percentage scale, with which that word appears in the message. Example: for a particular message, `word_make=0.21` means that 0.21% of the words in the message match the word 'make'.\n",
    "\n",
    "* 3 numeric features indicating, respectively, the **average length** of uninterrupted sequences of capital letters, the length of the longest uninterrupted sequence of capital letters and the total number of capital letters in the message.\n",
    "\n",
    "* A label 1/0 indicating whether that e-mail is spam (`spam`).\n",
    "\n",
    "### Importing the data\n",
    "\n",
    "I import the data with the `pandas` function `read_csv`. The data set has 4,601 rows and 52 columns, all numeric. I print the structure of the data set only for the first and the last five variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fname = 'https://raw.githubusercontent.com/mcanela-iese/ML_Course/master/Data/' \\\n",
    "     'spam.csv'\n",
    "df = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 5 columns):\n",
      "word_make       4601 non-null float64\n",
      "word_address    4601 non-null float64\n",
      "word_all        4601 non-null float64\n",
      "word_3d         4601 non-null float64\n",
      "word_our        4601 non-null float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 179.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.iloc[:, 0:5].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 5 columns):\n",
      "word_conference    4601 non-null float64\n",
      "cap_ave            4601 non-null float64\n",
      "cap_long           4601 non-null int64\n",
      "cap_total          4601 non-null int64\n",
      "spam               4601 non-null int64\n",
      "dtypes: float64(2), int64(3)\n",
      "memory usage: 179.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.iloc[:, 47:52].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a decision tree\n",
    "\n",
    "I take last column of this data frame (`spam`) as the **target vector**. The other 51 columns form the **features matrix**. I specify them as usual in scikit-learn **supervised methods**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, 51]\n",
    "X = df.iloc[:, 0:51]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a decision tree is the same as fitting any other supervised learning model in `scikit-learn`. There are many ways of controlling the growth of the tree. To start the discussion, I want a small tree which can be  plotted and visually examined. So, I use `man_depth=2`, which gives me a tree of 4 leaves. For the rest of the arguments, which are printed below, I accept the default values. \n",
    "\n",
    "*Note*. Update scikit-learn to the current version if you to get all the results that follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "treeclf = tree.DecisionTreeClassifier(max_depth=2)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick evaluation is given in scikit-learn by the attribute `score`, which, in regression models, gives the R-squared statistic and, in classification models, the **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.834"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(treeclf.score(X, y), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the tree\n",
    "\n",
    "Before evaluating the classifier, I will show you the tree, for a better understanding of how it works. Of course, examining the tree only makes sense when the tree is small, so this is done here for didactic purposes only. There are two ways of showing explicitly a decision tree, text report and graphics.\n",
    "\n",
    "A text report can be obtained in `sklearn.tree` with the class `export_text`, as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- word_remove <= 0.01\n",
      "|   |--- word_free <= 0.13\n",
      "|   |   |--- class: 0\n",
      "|   |--- word_free >  0.13\n",
      "|   |   |--- class: 1\n",
      "|--- word_remove >  0.01\n",
      "|   |--- word_george <= 0.08\n",
      "|   |   |--- class: 1\n",
      "|   |--- word_george >  0.08\n",
      "|   |   |--- class: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tree.export_text(treeclf, feature_names=list(X.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the graphical presentation, I use the class `export_graphviz`, which exports the tree to a format (`dot`) which can be managed by **Graphviz**. Graphviz is an open source app, not part of Python, which you need to have installed in your computer to visualize the tree on your screen. The three last lines of the following code chunk are used for generating a PDF version of the DOT representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(treeclf, out_file=None,\n",
    "  feature_names=df.columns[0:51])\n",
    "# import pydotplus\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "# graph.write_pdf('tree.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that Graphviz is installed, I import the module `graphviz` and ask for the plot. Note: the first two lines are needed in Windows for Python to find the Graphviz executable. In Macintosh, installing Graphviz with Homebrew fixes this type of problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"529pt\" height=\"258pt\"\n",
       " viewBox=\"0.00 0.00 528.54 258.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 254)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-254 524.5371,-254 524.5371,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"345.8115,-250 204.7256,-250 204.7256,-186 345.8115,-186 345.8115,-250\"/>\n",
       "<text text-anchor=\"middle\" x=\"275.2686\" y=\"-234.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">word_remove &lt;= 0.01</text>\n",
       "<text text-anchor=\"middle\" x=\"275.2686\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.478</text>\n",
       "<text text-anchor=\"middle\" x=\"275.2686\" y=\"-206.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 4601</text>\n",
       "<text text-anchor=\"middle\" x=\"275.2686\" y=\"-192.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [2788, 1813]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"265.8065,-150 132.7306,-150 132.7306,-86 265.8065,-86 265.8065,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"199.2686\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">word_free &lt;= 0.135</text>\n",
       "<text text-anchor=\"middle\" x=\"199.2686\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.4</text>\n",
       "<text text-anchor=\"middle\" x=\"199.2686\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3794</text>\n",
       "<text text-anchor=\"middle\" x=\"199.2686\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [2745, 1049]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M250.8033,-185.8089C244.1588,-177.0661 236.8791,-167.4876 229.9386,-158.3553\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"232.5254,-155.9746 223.6879,-150.1308 226.9522,-160.2102 232.5254,-155.9746\"/>\n",
       "<text text-anchor=\"middle\" x=\"220.3101\" y=\"-170.7015\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"420.6758,-150 283.8613,-150 283.8613,-86 420.6758,-86 420.6758,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"352.2686\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">word_george &lt;= 0.08</text>\n",
       "<text text-anchor=\"middle\" x=\"352.2686\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.101</text>\n",
       "<text text-anchor=\"middle\" x=\"352.2686\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 807</text>\n",
       "<text text-anchor=\"middle\" x=\"352.2686\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [43, 764]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M300.0557,-185.8089C306.8564,-176.9769 314.3137,-167.2921 321.4101,-158.0759\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"324.2001,-160.1894 327.5279,-150.1308 318.6537,-155.9187 324.2001,-160.1894\"/>\n",
       "<text text-anchor=\"middle\" x=\"330.7494\" y=\"-170.7223\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"126.3065,-50 .2306,-50 .2306,0 126.3065,0 126.3065,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"63.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.291</text>\n",
       "<text text-anchor=\"middle\" x=\"63.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3108</text>\n",
       "<text text-anchor=\"middle\" x=\"63.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [2559, 549]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M152.4651,-85.9947C138.212,-76.2481 122.5785,-65.5575 108.4358,-55.8864\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.0784,-52.7696 99.8482,-50.014 106.1271,-58.5478 110.0784,-52.7696\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"263.8066,-50 144.7306,-50 144.7306,0 263.8066,0 263.8066,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"204.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.395</text>\n",
       "<text text-anchor=\"middle\" x=\"204.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 686</text>\n",
       "<text text-anchor=\"middle\" x=\"204.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [186, 500]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M200.9893,-85.9947C201.4391,-77.6273 201.9264,-68.5643 202.3843,-60.0478\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"205.8817,-60.1875 202.9237,-50.014 198.8918,-59.8117 205.8817,-60.1875\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"403.3066,-50 291.2305,-50 291.2305,0 403.3066,0 403.3066,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.066</text>\n",
       "<text text-anchor=\"middle\" x=\"347.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 791</text>\n",
       "<text text-anchor=\"middle\" x=\"347.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [27, 764]</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M350.5478,-85.9947C350.098,-77.6273 349.6107,-68.5643 349.1528,-60.0478\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"352.6453,-59.8117 348.6134,-50.014 345.6554,-60.1875 352.6453,-59.8117\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"520.3067,-50 422.2304,-50 422.2304,0 520.3067,0 520.3067,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"471.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"471.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 16</text>\n",
       "<text text-anchor=\"middle\" x=\"471.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [16, 0]</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M393.2216,-85.9947C405.4577,-76.432 418.8566,-65.9606 431.0454,-56.4349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"433.5373,-58.9295 439.2614,-50.014 429.2269,-53.4141 433.5373,-58.9295\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1161160d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "import graphviz\n",
    "graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is easy to interpret. The box on top (depth zero) is the root node, with 4,601 instances, of which 1,813 are spam. The current value of the Gini measure of impurity is 0.478. The **optimal split**, based on `word_remove`, is the one that yields the maximum impurity reduction, meaning the lowest value for the weighted average of the Gini values of the branches. At depth one, the current Gini value is\n",
    "\n",
    "$${\\rm gini} = {3794\\times 0.4 + 807 \\times 0.101 \\over 4601} = 0.348.$$\n",
    "\n",
    "We repeat the process at the two nodes at depth one. The optimal splits use the features `word_free` and `word_george`. The impurity keeps decreasing, and, in one of the nodes at depth two is already null, so if we relax the maximum depth constraint, this branch will not be further split.\n",
    "\n",
    "What are the predictions in the four leaves? Unless we change it by applying our own cutoff, they are based on majority (this would be equivalent to 0.5 cutoff). So, the predicted class in leaves 2 and 3 is spam and the predicted class in leaves 1 and 4 is non-spam. How good is this? Since, the objective is to develop a spam filter, our main concern is the false positive rate, that is, the proportion of legal that would be stopped by the filter. In this tree, we have 186 false positives in leave 2 and 27 false positives in leave 3, a total 213 (4.6%). Of course, the false negatives (549) also matter, but the main challenge is to reduce this 4.6% by allowing the tree to grow beyond depth two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "Plotting the tree and evaluating its predictive performance by visual inspection is only feasible for very small trees. So, we take now an approach based on the classification metrics provided by the module `metrics` of scikit-learn, in particular on the **confusion matrix** and the statistics derived from it. The matrix can be obtained directly the Pandas function `crosstab`, or using ad hoc resources from scikit-learn, which is what I do here.\n",
    "\n",
    "First, the predictions are extracted from the tree with the attribute `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = treeclf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I import the module `metrics` and calculate the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2575,  213],\n",
       "       [ 549, 1264]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "conf = metrics.confusion_matrix(y, y_pred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **precision** and the **recall** are given directly by the corresponding functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(metrics.precision_score(y, y_pred), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.697"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(metrics.recall_score(y, y_pred), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in this case, the data set has been artificially created, by joining two collections which come from different sources. So, the proportion of spam in the data is not the real one. This means that statistics like the accuracy and the precision do not make sense. Nevertheless, we can evaluate the classifier by examining the two rows of the confusion matrix separately, that is, looking at the **true positive rate**, which is the same as the recall, and **false positive rate**. If you are interested, you can get these statiscs directly with:\n",
    "\n",
    "The TP rate is not bad, meaning that 70% of the spam messages are filtered out, but the FP rate is a bit high for a spam filter. Can we improve these results with a better cutoff? Instead of exploring this with histograms, it is better to use crosstabulation here, since there are only 4 different scores, one for each leaf of the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.697"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tp = conf[1, 1]/np.sum(conf[1, :])\n",
    "round(tp, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.076"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = conf[0, 1]/np.sum(conf[0, :])\n",
    "round(fp, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the growth of the tree\n",
    "\n",
    "The growth of the tree can be controlled in many ways in the class `DecisionTreeClassifier`. Since I started using the maximum depth, I continue with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeclf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2598,  190],\n",
       "       [ 406, 1407]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = treeclf.predict(X)\n",
    "conf = metrics.confusion_matrix(y, y_pred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement is clear, both in false positives and false negatives. The statistics are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.776"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = conf[1, 1]/np.sum(conf[1, :])\n",
    "round(tp, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.068"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = conf[0, 1]/np.sum(conf[0, :])\n",
    "round(fp, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix looks a bit better, but we should check that this does not come at the price of **overfitting**. I leave that for the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "One of the advantages of decision tree classifiers is that it is very easy to get a report on the **importance** of every feature. The importance of a feature is computed as the proportion of impurity decrease brought by that feature. The attribute `feature_importances_` gives a vector containing importance values for all the features. Value zero signals that the corresponding feature is not used in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.50346408, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.28862986, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.13377951, 0.04335074,\n",
       "       0.        , 0.02464125, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00613457, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeclf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get these values sorted can help with big trees. When every feature involved in the tree is used only in one split, the importance ranking coincides with the order in which the features get involved in the generation of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_remove    0.503464\n",
       "word_free      0.288630\n",
       "word_money     0.133780\n",
       "word_hp        0.043351\n",
       "word_george    0.024641\n",
       "word_edu       0.006135\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(data=treeclf.feature_importances_, index=X.columns).sort_values(ascending=False).head(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
