{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "## Miguel √Ångel Canela, IESE Business School\n",
    "\n",
    "******\n",
    "\n",
    "###  What is a clustering algorithm?\n",
    "\n",
    "A clustering algorithm groups instances into **clusters**, based on their **similarity**. Clustering methods have been applied for long time in many fields, under specific names. For instance, in marketing, clustering customers is called **market segmentation**. In the machine learning context, clustering is presented as **unsupervised learning**. \n",
    "\n",
    "There are many clustering methods, all based on the same principle, to get the maximum similarity within clusters and the minimum similarity between clusters. This is operationalized through a **similarity measure**. Similarity measures are also used in other machine learning techniques, such as **kNN classification** or **collaborative filtering**.\n",
    "\n",
    "A warning note: clustering algorithms always produce clusters. The clusters extracted by the algorithm could be useless for their intended application. For instance, it you want them to help to understand your customers, they have to be described in a intelligible way. This would probably imply a low number of clusters. But, in e-commerce, you may not need to understand so much your customers, so you can be happy with a high number of segments. \n",
    "\n",
    "There are, basically, two approaches to clustering: the **distance-based** methods, such as the $k$-means and the nearest neighbor algorithms, and the **probability-based** methods, such as the EM clustering algorithm. Only the $k$-means method is considered here, because the other two, in spite of their popularity in textbooks, have **scalability** problems, meaning that they do not work, or become very slow, with big data sets.\n",
    "\n",
    "### Similarities\n",
    "\n",
    "In the distance-based methods, the similarity of two instances is measured by a distance formula, which is usually the **Euclidean distance**. The Euclidean distance is the ordinary, real-life distance. If two points in a 3-dimensional space are $x = (2,-1,3)$ and $y = (2,2,4)$, their distance is\n",
    "\n",
    "$$\\hbox{dist}(x,y)=\\sqrt{\\big(2-2\\big)^2+\\big((-1)-2\\big)^2+\\big(3-4\\big)^2} = 1.414.$$\n",
    "\n",
    "The general formula, for an $n$-dimensional space, is\n",
    "\n",
    "$$\\hbox{dist}(x,y)=\\sqrt{\\big(x_1-y_1\\big)^2+\\cdots+\\big(x_n-y_n\\big)^2}.$$\n",
    "\n",
    "This formula can be applied to any pair of rows of a data set. The dimension $n$ would then be the number of clustering attributes. The Euclidean distance is the default everywhere, but, in particular contexts, such as **text mining**, other measures like the **cosine-based** similarity are preferred.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Formulas like the Euclidean distance may give extra influence on the clustering process to some particular features, due to their higher variation on the actual data. To prevent this, the clustering variables are typically normalized. The **min-max normalization** is typical in this context, although statisticians prefer the **$z$-score transformation** (subtracting the mean and dividing by the standard deviation).\n",
    "\n",
    "In the min-max normalization, the features are forced, through a linear transformation, into the 0--1 range. If $X$ is the feature in its original scale, the formula for this transformation is\n",
    "\n",
    "$$Z = {X-\\min(X)\\over \\max(X)-\\min(X)}\\,.$$\n",
    "\n",
    "Now, $Z$ is the same feature in the normalized scale. Machine learning software applications may apply the min-max normalization by default in many algorithms, without warning. In the $k$-means algorithm, for instance, features are typically normalized. So, it is recommended to check this in the technical documentation.\n",
    "\n",
    "### K-means clustering\n",
    "\n",
    "In the the **$k$-means algorithm**, every cluster is based on a **center**. The center is an artificial instance which is taken as the \"typical element\" of the cluster. The descriptions of the customer segments obtained by these methods are based on their centers. The centers can also be used to cluster instances in another data set.\n",
    "\n",
    "The $k$-means method is iterative. The user specifies $k$, which is the number of clusters. In the first step, a random choice of $k$ instances is performed. These instances are taken as the centers of the initial clusters. Then, every instance is assigned to the cluster whose center is the closest one (in the Euclidean distance). In the second step, the average of every cluster is taken as the new center and the instances are reassigned using the new centers as the initial ones in the first step. This is iterated until a prespecified stopping criterion is met.\n",
    "\n",
    "In the resulting partition, the average of a cluster, called the **centroid**, is taken as the center of the cluster. The centroids can be used to cluster instances in another data set, assigning each new instance to the cluster whose centroid is at the minimum distance.\n",
    "\n",
    "Despite its drawbacks, the $k$-means algorithm remains the most widely used clustering algorithm. It is simple, easily understandable and reasonably scalable, and can be easily modified to deal with streaming data. To deal with very large data sets, substantial effort has also been made to further speed up $k$-means. In scikit-learn, it is provided by the class `KMeans` of the module `cluster`.\n",
    "\n",
    "*Note*. Due to the random start, two runs of the $k$-means algorithms can give (slightly) different results.\n",
    "\n",
    "### References\n",
    "\n",
    "1. DT Larose (2005), *Discovering Knowledge in Data*, Wiley\n",
    "\n",
    "2. F Provost & T Fawcett (2013), *Data Science for Business --- What You Need to Know About Data Mining and Data-Analytic Thinking*, O'Reilly.\n",
    "\n",
    "3. scikit-learn user guide (2015)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
