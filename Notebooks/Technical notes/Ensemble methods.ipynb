{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "## Miguel Ángel Canela, IESE Business School\n",
    "\n",
    "******\n",
    "\n",
    "### Ensemble methods\n",
    "\n",
    "Ask a complex question to thousands of random people, and then aggregate their answers. In many cases, you will find that this aggregated answer is better than an expert's answer. This has been called the **wisdom of the crowds**. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an **ensemble**. Thus, this technique is called ensemble learning. In scikit-learn, the module `sklearn.ensemble` offers a wide choice of methods.\n",
    "\n",
    "Suppose that you have trained a few classifiers, each one achieving about 80% accuracy. You may have a logistic regression classifier, a decision tree classifier, and perhaps a few more. A simple way to create a better classifier is to aggregate the predictions of these classifiers and predict the class getting more votes. This majority-vote classifier is called a **hard voting** classifier. Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a **weak learner** (meaning it does only slightly better than random guessing), the ensemble can still be a **strong learner** (achieving high accuracy), provided that there are enough weak learners and they are diverse enough. If all classifiers are able to estimate class probabilities (in scikit-learn, this is done with a `predict_proba` method), then you can predict the class with the highest class probability, averaged over all the individual classifiers. This is called **soft voting**. It often achieves higher performance than hard voting because it gives more weight to highly confident votes.\n",
    "\n",
    "### Bagging and pasting\n",
    "\n",
    "One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called **bagging** (short for bootstrap aggregating). When sampling is performed without replacement, it is called **pasting**.\n",
    "\n",
    "Bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor. The star of bagging ensemble methods is the random forest, which is the only one discussed here.\n",
    "\n",
    "### Random forests\n",
    "\n",
    "A **random forest** is an ensemble of decision trees, generally trained via bagging. The random forest algorithm introduces extra randomness when growing trees. Instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater tree diversity, generally yielding an overall better model. Despite its simplicity, the random forest is one of the most powerful ML algorithms available today. In general, random forest algorithms do not have overfitting problems, which are frequent in overgrown decision trees. \n",
    "\n",
    "Random forests can be used for both regression and classification. In scikit-learn, this is provided by the classes `ensemble.RandomForestRegressor` and `ensemble.RandomForestClassifier`. We control the growth the trees that form the forest as we do for individual trees, with arguments such as `max_depth` and `max_leaf_nodes`. We can also control `n_estimators`, which is the number of trees in the forest.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "The general idea of most **boosting** methods is to train predictors sequentially, each trying to correct its predecessor. Among the many boosting methods available, the most popular are **AdaBoost** (short for adaptive boosting) and **gradient boosting**. These notes do not cover AdaBoost.\n",
    "\n",
    "Gradient boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. It can also be used for regression and classification.  As in the random forest, the weak learners are decision trees. Gradient boosting fits the new predictor to the errors made by the previous predictor, be it a regressor or a classifier. \n",
    "\n",
    "In scikit-learn, gradient boosting is provided by the classes `GradientBoostingRegressor` and `GradientBoostingClassifier` of the module `ensemble`. You have arguments such as `min_impurity_decrease` and `max_depth`, to control the growth of the trees, `n_estimators`, for the number of trees, and `learning_rate`, which controls the contribution of each tree. Gradient boosting is fairly robust to overfitting, so a large number of trees usually results in better performance.\n",
    "\n",
    "**XGBoost** (extreme gradient boosting) is an algorithm that has recently been dominating applied machine learning competitions. It is an implementation of gradient boosted decision trees designed for speed and performance. In Python, it is available in the module `xgboost`, which can be used as if it were a scikit-learn module. The only difference is that you import it directly, not from `sklearn`. So, the class `ensemble.GradientBoostingRegressor` is replaced by `xgboost.XGBRegressor`, and the same for the classifier class. Typically, gradient boosting users optimize the algorithm, by playing with `learning_rate` and `n_iterations`. This takes less time with `xgboost` than with the scikit-learn module.  \n",
    "\n",
    "### References\n",
    "\n",
    "1. A Géron (2017), *Hands-On Machine Learning with Scikit-Learn and TensorFlow --- Concepts, Tools, and Techniques to Build Intelligent Systems*, O'Reilly.\n",
    "\n",
    "2. scikit-learn user guide (2015)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
