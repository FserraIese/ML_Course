{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "## Miguel Ángel Canela, IESE Business School\n",
    "\n",
    "******\n",
    "\n",
    "###  What is a decision tree?\n",
    "\n",
    "A **decision tree** is a collection of **decision nodes**, connected by branches, extending downwards from the **root node**, until terminating in the **leaf nodes**. **Binary trees** contain two branches per decision node. Any tree can be redesigned as a binary tree. All the trees that appear in this course are binary. The usual graphical representation of a decision tree puts the root on top and the leaves at the bottom, as in Figure 1, which has been built with Python and exported to PDF format with **Graphviz**.\n",
    "\n",
    "******\n",
    "\n",
    "### Figure 1\n",
    "\n",
    "![](Figures/tree.jpg)\n",
    "\n",
    "******\n",
    "\n",
    "Decision trees can be used for classification and regression purposes. A decision tree creates a partition of the data set into a collection of subsets, one for each leaf. In a predictive model based on a single decision tree, the predicted value is the same for all the instances of the same leaf. In a decision tree regressor, the predicted value is obtained in a straightforward way, as the average value of the target on that leaf. In a decision tree classifier, the predicted class is the one that happens more frequently in that leaf. \n",
    "\n",
    "Under the hood, the decision tree classifier predicts a probability for the instances in a particular leaf to to belong to each class. This predicted probability is the proportion of that class in the leaf. In a binary classifier, if the probability of the positive class is taken as a score, the prediction given by the model results from applying a cutoff 0.5 to the score.\n",
    "\n",
    "When developing a decision tree model for binary classification, if the positive and negative class are coded as 1 and 0, respectively, you can choose between two possibilities:\n",
    "\n",
    "* Developing a decision tree regressor, taking the predicted values as the scores and applying a cutoff.\n",
    "\n",
    "* Developing a decision tree classifier, which gives directly the predicted classes. \n",
    "\n",
    "*Note*. The decision trees discussed here are not the same as those used in decision analysis, in which there are two types of nodes, event nodes and decision nodes.\n",
    "\n",
    "### The CART algorithm\n",
    "\n",
    "The top popular method for developing decision tree models is **CART** (Classification And Regression Trees), used in the R package `rpart` and in scikit-learn, where the module `tree` provides the classes `DecisionTree Regressor` and `DecisionTreeClassifier`. \n",
    "\n",
    "In scikit-learn, the features are numeric. At every decision node, there is a **split**, which results from one of the features and a cutoff. CART chooses at every node the **optimal split**. In decision tree regressors, the split search is based on a **least squares** criterion. The difference between the actual target vale and the predicted target value is taken as a residual. The, the optimal split is the one for which the sum of squared residuals is minimum.\n",
    "\n",
    "In decision tree classifiers, the split is based either on the **Gini impurity measure** or on the **entropy**. The first one (the default) is defined as follows. Suppose that there are $k$ classes, and we have a set of instances in which the proportions of these classes are $p_1, p_2, \\dots, p_k$. The Gini value is\n",
    "\n",
    "$${\\rm Gini} = p_1\\big(1-p_1\\big) + p_2\\big(1-p_2\\big) + \\cdots + p_k\\big(1-p_k\\big).$$\n",
    "\n",
    "It is easy to see that the Gini value is maximum when $p_1=p_2=\\cdots=p_k$, and minimum (i.e. null) when all the instances belong to the same class. For every possible split, CART calculates the Gini value as the weighted average of the Gini values of the two branches, choosing the split that leads to minimum Gini value. This is called **impurity decrease**.\n",
    "\n",
    "### Controlling the growth of the tree\n",
    "\n",
    "**Overfitting** is a typical problem of predictive models. It occurs when a predictive model fits satisfactorily the data used to obtain it but fails with data which have not been used in the obtention of the model. This typically happens with very complex models if the data set is not big enough. Even with a moderate number of features (with $k$ binary features, we can have $2^k$ leaves), a tree whose growth is not stopped can lead to a complex model, so tree classifiers are prone to overfitting.\n",
    "\n",
    "scikit-learn has many ways that control the growth of the tree: `max_depth`, `max_leaf_nodes`, `min_samples_split`, `min_samples_leaf`, `min_impurity_decrease`, etc. We use only the first two in the examples of this course:\n",
    "\n",
    "* The **depth** of a tree is the number of nodes in the longest branch, not including the root node. If the maximum depth is set to $d$, the maximum number of leaves possible is $2^d$. The tree of Figure 1, with 4 nodes, has depth 3.\n",
    "\n",
    "* `max_leaf_nodes` controls directly the nunber of leaves. The tree of Figure 1 has been obtained setting `max_leaf_nodes=4`.\n",
    "\n",
    "### References\n",
    "\n",
    "1. A Géron (2017), *Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems*, O'Reilly.\n",
    "\n",
    "2. F Provost & T Fawcett (2013), *Data Science for Business --- What You Need to Know About Data Mining and Data-Analytic Thinking*, O'Reilly.\n",
    "\n",
    "3. scikit-learn user guide (2015)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
