{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "## Miguel Ángel Canela, IESE Business School\n",
    "\n",
    "******\n",
    "\n",
    "###  Dimensionality reduction\n",
    "\n",
    "Many machine learning problems involve a high number (say thousands) of features. This not only makes training extremely slow, but also makes it much harder to find a good solution. This problem is often referred to as the **curse of dimensionality** (see Géron, 2017, for a short mathematical discussion). Sometimes the problem can be addressed through **dimensionality reduction**. \n",
    "\n",
    "Broadly speaking, the aim of dimensionality reduction is to replace a set of $k$ numeric variables by a much smaller set which can do the same job. Of course, one can reduce dimensionality by dropping some features which are very strongly correlated with other features (statisticians call this multicollinearity), which makes them redundant. Dimensionality methods are more proactive, replacing the original features by new features, derived from the original ones by means of mathematical expressions, which, in the method described in this note, are linear. So, dimensionality reduction is one of the various approaches to **feature engineering**, which refers to the obtention of new features from the old ones, to facilitate the learning process.\n",
    "\n",
    "Reducing dimensionality does lose some information (just like compressing music WAV files to MP3 can degrade the sound quality), so even though it will speed up training, it may also make your system perform slightly worse. It also makes your pipelines a bit more complex and thus harder to maintain. So you should first try to train your system with the original data before considering using dimensionality reduction. In some cases, however, reducing the dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher performance (but in general it will not, it will just speed up training).\n",
    "\n",
    "Apart from speeding up training, dimensionality reduction is also extremely useful for **data visualization**. Reducing the number of dimensions down to two (or three) makes it possible to plot a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters. Some dimensionality reduction methods can also be used for **anonymization**: since the new features do not correspond to observable magnitudes, the identification of the instances is more difficult.\n",
    "\n",
    "There are two main approaches to reducing dimensionality, projection and manifold learning, both available in scikit-learn. **Manifold learning** is not discussed in this note. **Projection methods** use the correlation between the features to develop a smaller set of features by getting rid of the redundant information. Like linear regression, it they are, essentially, linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "\n",
    "**Principal component analysis** (PCA) is a classical projection technique with applications in many fields, not discussed here. This note is focused on the application to dimensionality reduction. PCA develops a new set of variables, which are obtained as linear functions of the original variables. These new variables are typically centered (zero mean) and uncorrelated. In some applications (such as factor analysis in multivariate statistics), they are also standardized, that is, they also have unit variance. In this note, and in the example that follows, which uses the class `PCA` of the scikit-learn module `decomposition`, the principal components are not standardized.\n",
    "\n",
    "In all variants of dimensionality reduction, the new variables are produced by an optimization algorithm: the first one is optimal, the second one is optimal among those uncorrelated with the first one, the third one is optimal among those uncorrelated to the first two, etc. The reduction is achieved by selecting the top-$p$ ones ($p$ is typically much lower than $k$).\n",
    "\n",
    "Let us be a bit more specific about the optimality of principal components. Let $X_1$, $\\dots$, $X_k$ be the original features, and $\\bar x_1$, $\\dots$, $\\bar x_k$ the respective means in the training set. The first principal component $C$ has the following property: among all the normalized combinations,\n",
    "\n",
    "$$C=a_1(X_1-\\bar x_1)+\\cdots+a_k(X_k-\\bar x_k),\\qquad a_1^2+\\cdots+a_k^2=1,$$\n",
    "\n",
    "it has the maximum variance. Now, the second principal component has the same properties, but restricting the search to the normalized combinations that are uncorrelated with the first component. Next, the third component is uncorrelated to the first two components and has maximum variance, and so on.\n",
    "\n",
    "This optimization problem can be easily solved with a bit of matrix algebra, by means of a factorization technique called **singular value decomposition** (SVD), available in NumPy. You do not really need to know much about this, since PCA algorithms are ready for you, but some of the mathematical terminology still pervades in the technical documentation.\n",
    "\n",
    "Applying PCA to dimensionality reduction is pretty easy. The only point is: how do we decide the value of $p$? Examining the variances of the principal components can help us. Suppose that the principal components are $C_1, \\dots, C_k$. The algorithm extracts the components so that \n",
    "\n",
    "$${\\rm var}(C_1)\\ge{\\rm var}(C_2)\\ge\\cdots\\ge{\\rm var}(C_k).$$\n",
    "\n",
    "Because of a mathematical property of the SVD factors, the sum of the variances of the principal components equals the sum of the variances of the original variables. So, PCA can be regarded as a decomposition of the **total variance** across the principal components:\n",
    "\n",
    "$$\\hbox{Total variance}={\\rm var}(X_1)+\\cdots+{\\rm var}(X_k)={\\rm var}(C_1)+\\cdots+{\\rm var}(C_k).$$\n",
    "\n",
    "Then, the proportion of the total variance explained by the first $p$ components is \n",
    "\n",
    "$${{\\rm var}(C_1)+\\cdots+{\\rm var}(C_p)\\over \\hbox{Total variance}}\\,.$$\n",
    "\n",
    "PCA users typically choose $p$ so that this proportion is relevant (say 80%).\n",
    "\n",
    "### References\n",
    "\n",
    "1. A Géron (2017), *Hands-On Machine Learning with Scikit-Learn and TensorFlow --- Concepts, Tools, and Techniques to Build Intelligent Systems*, O'Reilly.\n",
    "\n",
    "2. DT Larose (2005), *Data Mining and Predictive Analytics*, Wiley.\n",
    "\n",
    "3. scikit-learn user guide (2015)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
